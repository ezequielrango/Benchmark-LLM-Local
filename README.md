# ðŸš€ Benchmark Local LLM

Benchmark para medir el rendimiento de un modelo **Ollama** corriendo en local.

---

## âœ… Requisitos previos

1. Python 3.8+
2. Tener **Ollama** corriendo en el host (`ollama run <model>`)
3. Instalar dependencias:
   ```bash
   python3 -m pip install -r requirements.txt
